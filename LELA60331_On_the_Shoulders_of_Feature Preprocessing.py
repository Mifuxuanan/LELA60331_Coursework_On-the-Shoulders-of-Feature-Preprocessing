# -*- coding: utf-8 -*-
"""LELA60331_On the Shoulders of Features

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JowJ3ogTl2k6ZrKagB7XXZVVq-b-Yj-W

# **An NLP system for the classification of product reviews**

## **Multinomial logistic regression**
"""

!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt

reviews=[]
sentiment_ratings=[]
product_types=[]
helpfulness_ratings=[]

with open("Compiled_Reviews.txt") as f:
  for line in f.readlines()[1:]:
    fields=line.rstrip().split('\t')
    reviews.append(fields[0])
    sentiment_ratings.append(fields[1])
    product_types.append(fields[2])
    helpfulness_ratings.append(fields[3])

"""## **A. Dataset Analysis**"""

def find_ranges(dataset,target):
  """
  Function:
    1. Analyze the distribution of the raw dataset
  Inputs:
    dataset: The raw dataset
    target: The target class
  Return:
    ranges: The ranges of different classes in the dataset
  """
  ranges=[]
  start=None

  for i,value in enumerate(dataset):
    if value==target:
      if start is None:
        start=i
    elif start is not None:
      ranges.append(start if start==i-1 else (start,i-1))
      start=None

  if start is not None:
    ranges.append(start if start==len(dataset)-1 else (start,len(dataset)-1))

  return ranges

def Count(dataset):
  """
  Function:
    1. Count the number of different classes in the raw dataset to know their distributions
  Inputs:
    dataset: The raw dataset
  Return:
    results: A list of the rank of different classes
  """
  Counts=Counter(dataset)
  results=[]
  for label,count in Counts.items():
    ranges=find_ranges(dataset,label)
    results.append((label,count,ranges))
  return results

sentiment_results=Count(sentiment_ratings)
product_results=Count(product_types)
helpfulness_results=Count(helpfulness_ratings)

def table(results,title):
  """
  Function:
    1. Draw the table about the information in former function
  Inputs:
    results: A list of the rank of different classes
    title: The title of the table
  """
  print(f"{title}\n")
  print("{:<30} {:<10} {:<20}".format("Type","Count","Range"))
  print("-"*60)
  for label,count,ranges in results:
    print(f"{label:<30} {count:<10} {ranges}")
  print("\n")

def Plot(results,title,size=(3.5,4),color='skyblue',bar_width=0.8,fontsize_title=12,fontsize_labels=10,fontweight='bold',rotation=0,ha="center",start_step=0,end_step=2200):
  """
  Function:
    1. Draw the bar chart about the distribution of different classes
  Inputs:
    results: A list of the rank of different classes
    title: The title of the bar chart
    size: The size of the bar chart
    color: The color of the bar
    bar_width: The bin width of the bar
    fontsize_title: The font size of the title
    fontsize_labels: The font size of the x and y labels
    fontweight: The fontweight of the title and the x and y labels
    rotation & ha: How to put the name of different classes under the y-axis
    start_step & end_step: The range of the y-axis
  """
  labels=[r[0] for r in results]
  counts=[r[1] for r in results]
  plt.figure(figsize=size)
  plt.bar(labels,counts,color=color,width=bar_width)
  plt.title(title,fontsize=fontsize_title,fontweight=fontweight)
  plt.xlabel('Labels',fontsize=fontsize_labels,fontweight=fontweight)
  plt.ylabel('Counts',fontsize=fontsize_labels,fontweight=fontweight)
  plt.xticks(labels,rotation=rotation,ha=ha)
  for i,c in enumerate(counts):
    plt.text(i,c+0.5,str(c),ha='center',va='bottom',fontsize=10)
  plt.ylim(start_step,end_step)
  plt.tight_layout()
  plt.show()

Plot(sentiment_results,"The Distribution of Sentiment Ratings",color='salmon',start_step=10000,end_step=23000)
table(sentiment_results,"Sentiment Ratings")

Plot(product_results,"The Distribution of Product Types",size=(11,6),fontsize_title=16,fontsize_labels=12,rotation=45,ha="right")
table(product_results,"Product Types")

Plot(helpfulness_results,"The Distribution of Helpfulness Ratings",color='lightgreen',fontsize_title=12,fontsize_labels=10,start_step=2500,end_step=22000)
table(helpfulness_results,"Helpfulness Ratings")

"""## **B. libraries and information**"""

import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
import re
import math
import random

"""
A list of the stop words cited from the github "NLTK's list of english stopwords"
The related website: https://gist.github.com/sebleier/554280
"""
stop_words={"i","me","my","myself","we","our","ours","ourselves","you","your",
      "yours","yourself","yourselves","he","him","his","himself","she","her",
      "hers","herself","it","its","itself","they","them","their","theirs",
      "themselves","what","which","who","whom","this","that","these","those",
      "am","is","are","was","were","be","been","being","have","has","had",
      "having","do","does","did","doing","a","an","the","and","but","if",
      "or","because","as","until","while","of","at","by","for","with",
      "about","against","between","into","through","during","before","after",
      "above","below","to","from","up","down","in","out","on","off","over",
      "under","again","further","then","once","here","there","when","where",
      "why","how","all","any","both","each","few","more","most","other",
      "some","such","no","nor","not","only","own","same","so","than",
      "too","very","s","t","can","will","just","don","should","now"}

stop_words.update({"amazon","would","etc","got","go","went","thing","also","ve","m","d"}) # a custom set for Amazon reviews
punctuation={".",",","!","?",";",":","-","_","*","(",")","[","]","{","}","'s",
      "'re","&","#","'ve","'m","'ll","'d","\\","/","'","\"","+"}
keep_words={"no","nor","not"} # Remove negetive words for the sentiment anaylsis
stopwords_list=(stop_words|punctuation)-keep_words

"""## **C. Data Preprocessing**"""

def preprocess_text(text,regex='[^ ]+',toptoken_num=2500,
          apply_stopwords_list=False,stopwords=stopwords_list,n=1):
  """
  Function:
    1. Apply the regular expression to the dataset of reviews for word segmentation
    2. Apply the stopwords list to the dataset
    3. Apply the bigram logic to the dataset
    4. Count the frequency of tokens in the dataset and restore a list of the most frequent tokens
  Inputs:
    text: the raw text of reviews
    regex: A regular expression to decide how to tokenize the dataset
    toptoken_num: The number of the most frequent tokens to be used
    apply_stopwords_list: A boolean value to decide whether the dataset need to fliter stopwords
    stopwords: Load the set of stopwords
    n: A value to decide use unigram or bigram
  Return:
    newtext: The preprocessed dataset of reviews
    token_list: The preprocessed dataset of all tokens
    toptoken_list: The preprocessed dataset of the most frequent tokens
  """
  newtext=[]
  token_list=[]
  for txt in text:
    words=re.findall(regex,txt.lower())
    if apply_stopwords_list:
      words=[w for w in words if w not in stopwords]
    if n>1 and len(words)>=n:
      words=[' '.join(words[i:i+n]) for i in range(len(words)-n+1)]
    newtext.append(words)

  for s in newtext:
    token_list.extend(s)

  counts=Counter(token_list)
  counts_sort=sorted(counts.items(),key=lambda x:x[1],reverse=True)
  token_freq=list(zip(*counts_sort))
  token=token_freq[0]
  toptoken_list=token[:toptoken_num]

  return newtext,token_list,toptoken_list

def feature_coding(newtext,type_list,TF_IDF=False):
  """
  Function:
    1. Build the matrix of features by using TF-IDF vector representation
    2. Build the one-hot encoding matrix of features
  Inputs:
    newtext: The preprocessed dataset of reviews
    type_list: The preprocessed dataset of the most frequent tokens
    TF_IDF: A boolean value to decide whether the matrix of dataset need to use the TF-IDF matrix
  Return:
    M: The preprocessed matrix of features
  """
  num_docs=len(newtext)
  num_tokens=len(type_list)
  M=np.zeros((num_docs, num_tokens))

  if TF_IDF==True:
    # calculate TF
    TF=np.zeros((num_docs,num_tokens))
    for i,doc in enumerate(newtext):
      token_count=Counter(doc)  # Word frequency of the current document
      total_tokens=len(doc)  # The total number of words in the current document
      for word in token_count:
        if word in type_list:
          j=type_list.index(word)
          TF[i,j]=token_count[word]/total_tokens

    # calculate IDF
    doc_count=np.zeros(num_tokens)
    for j,word in enumerate(type_list):
      doc_count[j]=sum(1 for doc in newtext if word in doc)
    IDF=np.log((num_docs+1)/(doc_count+1))  # Add 1 to avoid a denominator of 0

    # calculate TF-IDF
    M=TF*IDF

  else:
    for i,utt in enumerate(reviews):
      # Tokenise the current review:
      tokens=re.findall("[^ ]+",utt)
      # iterate over the words in our type list (the set of 5000 words):
      for j,t in enumerate(type_list):
        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.
        if t in tokens:
          M[i,j]=1
  return M

def tokenization(text,regex='[^ ]+',toptoken_num=2500,apply_stopwords_list=False,
         stopwords=stopwords_list,n=1,TF_IDF=False):
  """
  Function:
    1. realize the former functions to preprocess and tokenize the raw dataset of reviews
  Inputs:
    text: The raw dataset of reviews
    regex: A regular expression to decide how to tokenize the dataset
    toptoken_num: The number of the most frequent tokens to be used
    apply_stopwords_list: A boolean value to decide whether the dataset need to fliter stopwords
    stopwords: A set of stopwords
    n: A value to decide use unigram or bigram
    TF_IDF: A boolean value to decide whether the matrix of dataset need to use TF-IDF
  return:
    newtext: The preprocessed dataset of reviews
    tokens: The preprocessed dataset of tokens
    toptokens: The preprocessed dataset of the most frequent tokens
  """
  newtext,tokens,toptokens=preprocess_text(text,regex,toptoken_num,apply_stopwords_list,stopwords,n)
  M=feature_coding(newtext,toptokens,TF_IDF)
  return newtext,tokens,toptokens,M

def split_dataset(label_data,M):
  """
  Function:
    1. Split the label dataset of target tasks into training, development and test sets
    2. One-hot encoding of the labels
  Input:
    label_data: A list of labels corresponding to the dataset, which can be choosen as "sentiment_ratings", "product_types" or "helpfulness_ratings"
    M: The preprocessed dataset of features
  return:
    M_train: The training dataset of features
    M_test: The test dataset of features
    M_dev: The development dataset of features
  """
  train_ints=np.random.choice(len(label_data),int(len(label_data)*0.8),replace=False)
  remaining_ints=list(set(range(len(label_data)))-set(train_ints))
  test_ints=np.random.choice(remaining_ints,int(len(remaining_ints)*0.5),replace=False)
  dev_ints=list(set(remaining_ints)-set(test_ints))

  M_train=M[train_ints].T
  M_test=M[test_ints].T
  M_dev=M[dev_ints].T

  unique_labels=list(set(label_data))
  unique_one_hot=np.diag(np.ones(len(unique_labels)))

  labels_train=[label_data[i] for i in train_ints]
  labels_test=[label_data[i] for i in test_ints]
  labels_dev=[label_data[i] for i in dev_ints]

  y_train=np.array([unique_one_hot[unique_labels.index(x)] for x in labels_train]).T
  y_test=np.array([unique_one_hot[unique_labels.index(x)] for x in labels_test]).T
  y_dev=np.array([unique_one_hot[unique_labels.index(x)] for x in labels_dev]).T

  M_train=M_train.T
  M_test=M_test.T
  M_dev=M_dev.T
  y_train=y_train.T
  y_test=y_test.T
  y_dev=y_dev.T

  return M_train,M_test,M_dev,y_train,y_test,y_dev

newtext,tokens,toptokens,M=tokenization(reviews,regex='[^ ]+',toptoken_num=2500,apply_stopwords_list=False,
         stopwords=stopwords_list,n=1,TF_IDF=False)

M_train,M_test,M_dev,y_train,y_test,y_dev=split_dataset(product_types,M)
print("M_train stats: min =", np.min(M_train), ", max =", np.max(M_train))
print(M_train.shape)
print(y_train.shape)
y_train

"""## **D. Model Training**"""

def training(M_train,y_train,k=32,lr=0.001,n_iters=200,num_features=2500):
  """
  Function:
    1. Train the logistic regression model
  Inputs:
    M_train: The preprocessed dataset of features
    y_train: The preprocessed dataset of labels
    k: batch size
    lr: learning rate
    n_iters: Number of Iterations
    num_features: Number of features
  return:
    weights: The trained weights matrix
    logistic_loss: The loss of logistic regression
  """
  indices=np.arange(y_train.shape[0])
  np.random.shuffle(indices)
  batches=np.array_split(indices,k)

  np.random.seed(10)
  num_classes=y_train.shape[1]
  logistic_loss=[]
  weights=np.random.rand(num_features,num_classes)
  epsilon=1e-10

  # batch training
  for i in range(n_iters):
    loss=0.0
    for b in batches:
      X_batch=M_train[b]
      y_batch=y_train[b]
      if X_batch.shape[0]==0:
        continue
      z=X_batch.dot(weights)
      z_sum=np.exp(z).sum(axis=1,keepdims=True)
      q=np.array([np.exp(z_i)/z_sum[i] for i,z_i in enumerate(z)])
      loss+=np.mean(-np.log2(np.sum(y_batch*q,axis=1)+epsilon))# Add epsilon to avoid log with base zero

      dw=X_batch.T.dot(q-y_batch)
      weights-=lr*dw

    logistic_loss.append(loss)
  print("Final Weights Matrix Shape:", weights.shape)
  print("Final Weights Matrix(sample):", weights[:5, :5])

  plt.plot(range(n_iters),logistic_loss,label="Logistic Loss")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.legend()
  plt.show()

  return weights,logistic_loss

"""## **E. Model Evaluattion**"""

def evaluation(M_set,y_set,weights,epsilon=1e-10):
  """
  Function:
    1. Evaluate the logistic regression model by the rates of precision, recall, macroaverage precision and macroaverage recall
  Inputs:
    M_set: The preprocessed dataset of features
    y_set: The preprocessed dataset of labels
    weights: The trained weights matrix
    epsilon: An additional value to avoid a denominator of 0 when calculating the rates of precision and recall
  return:
    precision: Precision rate
    recall: Recall rate
    p_macro: Macroaverage precision
    r_macro: Macroaverage recall
  """
  z=M_set.dot(weights)
  z_sum=np.exp(z).sum(axis=1,keepdims=True)
  q=np.array([np.exp(z_i)/z_sum[i] for i,z_i in enumerate(z)])
  pred=np.argmax(q,axis=1)
  true=np.argmax(y_set,axis=1)

  num_classes=y_set.shape[1]
  TP,FP,FN=[],[],[]
  for j in range(num_classes):
    TP.append(np.sum((pred==j) & (true==j)))
    FP.append(np.sum((pred==j) & (true!=j)))
    FN.append(np.sum((pred !=j) & (true==j)))

  precision=np.array(TP)/(np.array(TP)+np.array(FP)+epsilon) # Add epsilon to avoid a denominator of 0
  recall=np.array(TP)/(np.array(TP)+np.array(FN)+epsilon)
  print('Precision:',precision)
  print('Recall:',recall)

  p_macro=precision.mean()
  r_macro=recall.mean()
  print('Macroaverage Precision:',p_macro)
  print('Macroaverage Recall:',r_macro)

  return precision,recall,p_macro,r_macro

"""## **F. Main fuction**"""

def main(label_data,apply_stopwords_list=False,n=1,TF_IDF=False,k=32,
     lr=0.001,n_iters=200,toptoken_num=2500,regex='[^ ]+',eval_set="test"):
  """
  Function:
    1. Call all the former functions
  Inputs:
    label_data: A list of labels corresponding to the dataset, which can be choosen as "sentiment_ratings", "product_types" or "helpfulness_ratings"
    apply_stopwords_list: A boolean value to decide whether the dataset need to fliter stopwords
    n: A value to decide use unigram or bigram
    TF_IDF: A boolean value to decide whether the matrix of dataset need to use TF-IDF
    toptoken_num: The number of the most frequent tokens to be used
    regex: A regular expression to decide how to tokenize the dataset
    k: batch size
    lr: learning rate
    n_iters: Number of Iterations
    eval_set: Specify which set to evaluate (use"train" for training set, use "test" for test set, or use "dev" for development set)
  return:
    precision: Precision rate of the target dataset
    recall: Recall rate of the target dataset
    p_macro: Macroaverage precision of the target dataset
    r_macro: Macroaverage recall of the target dataset
  """
  a,b,c,M=tokenization(reviews,regex=regex,toptoken_num=toptoken_num,
            apply_stopwords_list=apply_stopwords_list,
            stopwords=stopwords_list,n=n,TF_IDF=TF_IDF)

  M_train,M_test,M_dev,y_train,y_test,y_dev=split_dataset(label_data,M)

  weights,logistic_loss=training(M_train,y_train,k=k,lr=lr,n_iters=n_iters,
                 num_features=M_train.shape[1])
  if eval_set=="train":
    M_eval,y_eval=M_train,y_train
  elif eval_set=="dev":
    M_eval,y_eval=M_dev,y_dev
  else:
    M_eval,y_eval=M_test,y_test

  precision,recall,p_macro,r_macro=evaluation(M_eval,y_eval,weights)
  return precision,recall,p_macro,r_macro

"""## **G. Tests on three tasks**
There are eight tests under each task. Each evaluation variables followed by two digitals:
- The first digital represents three classification tasks:
  
  1. Sentiment

  2. Product Types

  3. Helpfulness

- The second digital represents eight methods of feature engineering

  1. Unigram Tokenization
  2. Unigram Tokenization with TF-IDF Representation Matrix
  3. Unigram Tokenization with Stopwords flitering
  4. Unigram Tokenization with both TF-IDF Representation Matrix and Stopwords flitering
  5. Bigram Tokenization
  6. Bigram Tokenization with TF-IDF Representation Matrix
  7. Bigram Tokenization with Stopwords flitering
  8. Bigram Tokenization with both TF-IDF Representation Matrix and Stopwords flitering

### **1. Sentiment(8/8)**
"""

# Unigram Tokenization
precision11,recall11,p_macro11,r_macro11=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 5000 toptokens
precision11_5000,recall11_5000,p_macro11_5000,r_macro11_5000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=5000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 10000 toptokens
precision11_10000,recall11_10000,p_macro11_10000,r_macro11_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with TF-IDF
precision12,recall12,p_macro12,r_macro12=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with TF-IDF and 10000 toptokens
precision12_10000,recall12_10000,p_macro12_10000,r_macro12_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with stopwords flitering
precision13,recall13,p_macro13,r_macro13=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with stopwords flitering and 10000 toptokens
precision13,recall13,p_macro13,r_macro13=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with both TF-IDF and stopwords flitering
precision14,recall14,p_macro14,r_macro14=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with both TF-IDF and stopwords flitering and 10000 toptokens
precision14_10000,recall14_10000,p_macro14_10000,r_macro14_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram Tokenization
precision15,recall15,p_macro15,r_macro15=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram Tokenization with 10000 toptokens
precision15_10000,recall15_10000,p_macro15_10000,r_macro15_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with TF_IDF
precision16,recall16,p_macro16,r_macro16=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with TF_IDF and 10000 tokens
precision16_10000,recall1610000,p_macro1610000,r_macro1610000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with stopwords flitering
precision17,recall17,p_macro17,r_macro17=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with stopwords flitering and 10000 toptokens
precision17_10000,recall17_10000,p_macro17_10000,r_macro17_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with both TF-IDF and stopwords flitering
precision18,recall18,p_macro18,r_macro18=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with both TF-IDF and stopwords flitering and 10000 toptokens
precision18_10000,recall18_10000,p_macro18_10000,r_macro18_10000=main(label_data=sentiment_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

"""### **2. Product type((8/8))**"""

# Unigram Tokenization
precision21,recall21,p_macro21,r_macro21=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 5000 toptokens
precision21_5000,recall21_5000,p_macro21_5000,r_macro21_5000=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=5000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 10000 toptokens
precision21_10000,recall21_10000,p_macro21_10000,r_macro21_10000=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with TF-IDF
precision22,recall22,p_macro22,r_macro22=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with stopwords flitering
precision23,recall23,p_macro23,r_macro23=main(label_data=product_types,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with both TF-IDF and stopwords flitering
precision24,recall24,p_macro24,r_macro24=main(label_data=product_types,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram Tokenization
precision25,recall25,p_macro25,r_macro25=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with TF_IDF
precision26,recall26,p_macro26,r_macro26=main(label_data=product_types,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with stopwords flitering
precision27,recall27,p_macro27,r_macro27=main(label_data=product_types,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with both TF-IDF and stopwords flitering
precision28,recall28,p_macro28,r_macro28=main(label_data=product_types,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

"""### **3. Helpfulness(8/8)**"""

# Unigram Tokenization
precision31,recall31,p_macro31,r_macro31=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 5000 toptokens
precision31_5000,recall31_5000,p_macro31_5000,r_macro31_5000=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=5000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram Tokenization with 10000 toptokens
precision31_10000,recall31_10000,p_macro31_10000,r_macro31_10000=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=10000,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with TF-IDF
precision32,recall32,p_macro32,r_macro32=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with stopwords flitering
precision33,recall33,p_macro33,r_macro33=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Unigram with both TF-IDF and stopwords flitering
precision34,recall34,p_macro34,r_macro34=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=True,
                   n=1,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram Tokenization
precision35,recall35,p_macro35,r_macro35=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with TF_IDF
precision36,recall36,p_macro36,r_macro36=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=False,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with stopwords flitering
precision37,recall37,p_macro37,r_macro37=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=False,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")

# Bigram with both TF-IDF and stopwords flitering
precision38,recall38,p_macro38,r_macro38=main(label_data=helpfulness_ratings,
                   apply_stopwords_list=True,
                   n=2,
                   TF_IDF=True,
                   k=32,
                   lr=0.001,
                   n_iters=200,
                   toptoken_num=2500,
                   regex='[^ ]+',
                   eval_set="test")